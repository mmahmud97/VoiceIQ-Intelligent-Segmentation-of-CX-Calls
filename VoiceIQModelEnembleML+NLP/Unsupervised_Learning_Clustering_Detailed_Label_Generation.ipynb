{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans, SpectralClustering, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_excel(\"/Users/muzammil.mahmud/Desktop/VSCode/NLP Quick Win/files/Descriptions.xlsx\")\n",
    "df.dropna(subset=['Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "571c1879f8304a77974909c6ed32f6f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/10560 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"Start embeddings\")\n",
    "embeddings = model.encode(df['Description'].tolist(), show_progress_bar=True)\n",
    "print(\"End embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Algorithms\n",
    "def apply_kmeans(embeddings, n_clusters=50):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "    return kmeans.fit_predict(embeddings)\n",
    "\n",
    "def apply_mini_batch_kmeans(embeddings, n_clusters=50):\n",
    "    mbkmeans = MiniBatchKMeans(n_clusters=n_clusters, batch_size=1000, random_state=42)\n",
    "    return mbkmeans.fit_predict(embeddings)\n",
    "\n",
    "def apply_hdbscan(embeddings_samples, min_cluster_size=15):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size, gen_min_span_tree=True)\n",
    "    return clusterer.fit_predict(embeddings_samples)\n",
    "\n",
    "# # May not work since graph is not fully connected\n",
    "def apply_spectral_clustering(embeddings, n_clusters=100):\n",
    "    spectral = SpectralClustering(n_clusters=n_clusters, random_state=42, affinity='nearest_neighbors')\n",
    "    return spectral.fit_predict(embeddings)\n",
    "\n",
    "def apply_agglomerative_clustering(embeddings, n_clusters=100):\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    return agglomerative.fit_predict(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guide on Using Testing Mechanisms\n",
    "# Silhouette Score: Measures how similar an object is to its own cluster compared to other clusters. \n",
    "# The value ranges from -1 to 1, where a high value indicates that the object is well matched to its\n",
    "# own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "# Davies-Bouldin Index: The average 'similarity' between each cluster and its most similar cluster, \n",
    "# where similarity is the ratio of within-cluster distances to between-cluster distances. Lower values \n",
    "# indicate better clustering.\n",
    "\n",
    "# Calinski-Harabasz Index: Ratio of the sum of between-clusters dispersion and of within-cluster dispersion\n",
    "# for all clusters, where higher scores indicate clusters are dense and well separated.\n",
    "\n",
    "# The choice of metric depends on the specific characteristics of your data and the goals of your clustering. \n",
    "# Generally, it's good practice to look at multiple metrics to get a holistic view of your clustering's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming all necessary imports and functions (apply_mini_batch_kmeans, etc.) are defined above\n",
    "# Assuming df is your DataFrame loaded from 'Descriptions.xlsx'\n",
    "\n",
    "# Define your method to only include MiniBatchKMeans\n",
    "methods = {\n",
    "    'KMeans': apply_kmeans\n",
    "   # 'MiniBatchKMeans': apply_mini_batch_kmeans\n",
    "}\n",
    "\n",
    "for name, method in methods.items():\n",
    "    print(f\"Running {name}...\")\n",
    "\n",
    "    # Run MiniBatchKMeans clustering algorithm\n",
    "    labels_pred = method(embeddings, n_clusters=25)  # Adjust n_clusters as necessary\n",
    "\n",
    "    # Add the cluster labels as a new column to the DataFrame\n",
    "    #Kmeans stored in Cluster\n",
    "    df['Cluster2'] = labels_pred\n",
    "    #minibatchkemeans stored in Cluster\n",
    "    #df['Cluster'] = labels_pred\n",
    "\n",
    "    print(f\"Clustering completed for {name}. Cluster labels added to the DataFrame.\")\n",
    "    \n",
    "# At this point, df has a new column named 'Cluster' containing the cluster labels for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame with a 'Cluster' column from previous clustering\n",
    "# Assuming 'YourTextColumnNameHere' is the column name of the text data\n",
    "\n",
    "n_topics_per_cluster = 4  # Number of topics to find per cluster\n",
    "n_words_per_topic = 25  # Number of words to display for each topic\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "dominant_topics = []  # List to store dominant topic for each document\n",
    "\n",
    "# Optional: Dictionary to hold top words for each topic for summary\n",
    "topics_summary = {}\n",
    "\n",
    "for cluster_id in df['Cluster2'].unique():\n",
    "    documents_in_cluster = df[df['Cluster2'] == cluster_id]['Description']\n",
    "    if documents_in_cluster.empty:\n",
    "        continue  # Skip clusters with no documents\n",
    "\n",
    "    dtm = vectorizer.fit_transform(documents_in_cluster)\n",
    "    lda = LatentDirichletAllocation(n_components=n_topics_per_cluster, random_state=0)\n",
    "    lda.fit(dtm)\n",
    "\n",
    "    topic_distribution = lda.transform(dtm)\n",
    "    for i, topic_prob_dist in enumerate(topic_distribution):\n",
    "        dominant_topic = np.argmax(topic_prob_dist)\n",
    "        document_index = documents_in_cluster.index[i]\n",
    "        dominant_topics.append((document_index, cluster_id, dominant_topic))\n",
    "\n",
    "    # Optional: Summarize topics for the cluster\n",
    "    words = vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        topic_key = f\"Cluster {cluster_id}, Topic {topic_idx}\"\n",
    "        top_words = \" \".join(words[i] for i in topic.argsort()[:-n_words_per_topic - 1:-1])\n",
    "        topics_summary[topic_key] = top_words\n",
    "\n",
    "# Create a DataFrame from the dominant_topics list\n",
    "dominant_topics_df = pd.DataFrame(dominant_topics, columns=['Index', 'Cluster2', 'Dominant_Topic'])\n",
    "dominant_topics_df.set_index('Index', inplace=True)\n",
    "\n",
    "dominant_topics_df.drop(['Cluster2'], axis=1, inplace=True)\n",
    "# Merge the dominant topic information back into the original DataFrame\n",
    "df = df.merge(dominant_topics_df, left_index=True, right_index=True)\n",
    "\n",
    "# Optional: Save or print the topics_summary for reference\n",
    "print(topics_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Case Number', 'Description'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'topics_summary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# isolate all the new topics with Cluster Number and topic number. Output file should contain Cluster, Topic, and the text of the topic \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#for example: Cluster 43, Topic 2': 'employee file provide copy information employer personnel employees request required'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#It will be unique so we would only have records that correspond to the unique number of topics\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming topics_summary dictionary holds your topics\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m unique_topics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(\u001b[43mtopics_summary\u001b[49m\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic_Key\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic_Words\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      7\u001b[0m unique_topics_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munique_topics_for_gpt.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(unique_topics_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic_Key\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'topics_summary' is not defined"
     ]
    }
   ],
   "source": [
    "# isolate all the new topics with Cluster Number and topic number. Output file should contain Cluster, Topic, and the text of the topic \n",
    "#for example: Cluster 43, Topic 2': 'employee file provide copy information employer personnel employees request required'\n",
    "#It will be unique so we would only have records that correspond to the unique number of topics\n",
    "\n",
    "# Assuming topics_summary dictionary holds your topics\n",
    "unique_topics_df = pd.DataFrame(list(topics_summary.items()), columns=['Topic_Key', 'Topic_Words'])\n",
    "unique_topics_df.to_csv('unique_topics_for_gpt.csv', index=False)\n",
    "\n",
    "print(unique_topics_df['Topic_Key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to ChatGPT and run these models\n",
    "import pandas as pd\n",
    "import openai\n",
    "\n",
    "# Initialize the OpenAI client with your API key\n",
    "client = openai.OpenAI(api_key='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')\n",
    "\n",
    "def enhance_topic_label(topic_words):\n",
    "    # Make the call to the OpenAI API using the client\n",
    "    response = client.completions.create(\n",
    "        model=\"gpt-3.5-turbo-instruct\", # Adjust the model name as necessary\n",
    "        prompt=f\"Generate a label for these keywords from phone call transcript in most descriptive way in less than 12 words: {topic_words}\",\n",
    "        max_tokens=60,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# The rest of your script remains the same\n",
    "# Load unique topics DataFrame\n",
    "unique_topics_df = pd.read_csv('unique_topics_for_gpt.csv')\n",
    "\n",
    "# Generate enhanced labels for each unique topic\n",
    "unique_topics_df['Enhanced_Label'] = unique_topics_df['Topic_Words'].apply(enhance_topic_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_topics_df.head)\n",
    "print(df.columns)\n",
    "\n",
    "# Create a unified key in 'df' for merging\n",
    "df['Merge_Key'] = 'Cluster ' + df['Cluster2'].astype(str) + ', Topic ' + df['Dominant_Topic'].astype(str)\n",
    "\n",
    "# Merge 'unique_topics_df' into 'df' using the new keys\n",
    "df = df.merge(unique_topics_df, left_on='Merge_Key', right_on='Topic_Key', how='left')\n",
    "\n",
    "# Optionally, drop the temporary merge keys if they're no longer needed\n",
    "df.drop(['Merge_Key'], axis=1, inplace=True)\n",
    "\n",
    "# Optionally, rename the 'Enhanced_Label' column to something more descriptive if needed\n",
    "df.rename(columns={'Enhanced_Label': 'GPT_Improved_Labels'}, inplace=True)\n",
    "\n",
    "# Export the DataFrame to an Excel file\n",
    "df.to_excel('final_dataset_with_enhanced_topics_25_4_labels.xlsx', index=False)\n",
    "\n",
    "# # Merge enhanced labels back into the original DataFrame\n",
    "# df = df.merge(unique_topics_df, left_on='Dominant_Topic', right_on='Topic_Key', how='left')\n",
    "\n",
    "# # Optionally, rename the 'Enhanced_Label' column to something more descriptive\n",
    "# df.rename(columns={'Enhanced_Label': 'GPT_Improved_Labels'}, inplace=True)\n",
    "\n",
    "# # Share Results:\n",
    "# df.to_excel('final_dataset_with_enhanced_topics.xlsx', index=False)\n",
    "\n",
    "# print(unique_topics_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_excel(\"final_dataset_with_enhanced_topics_25_4_labels.xlsx\")\n",
    "label_frequencies = dd['GPT_Improved_Labels'].value_counts()\n",
    "\n",
    "# Output the frequencies\n",
    "print(label_frequencies)\n",
    "\n",
    "label_frequencies.to_excel(\"frequency_table_25_4.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')  # for stop words\n",
    "\n",
    "# Assuming 'df' is already loaded with your data\n",
    "df = pd.read_excel('clusterCheck.xlsx')  # Uncomment if you need to load the data\n",
    "\n",
    "# Get English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Initialize an empty list to hold all tokens\n",
    "all_tokens = []\n",
    "\n",
    "# Iterate over each transcript in the 'Description' column\n",
    "for description in df['Description']:\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(description.lower())  # Convert to lower case\n",
    "    # Remove stop words and non-alphabetic tokens\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "# Calculate frequencies\n",
    "word_frequencies = Counter(all_tokens)\n",
    "\n",
    "# Get the 50 most common words\n",
    "top_50_words = word_frequencies.most_common(100)\n",
    "\n",
    "# Output the 50 most common words\n",
    "print(top_50_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_words_only = [word for word, frequency in top_50_words]\n",
    "\n",
    "# Print the list of words\n",
    "print(top_50_words_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the datasets\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df2 \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal_dataset_with_enhanced_topics_2.xlsx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKCDataSet.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/io/excel/_base.py:508\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(\n    io,\n    sheet_name,\n    header,\n    names,\n    index_col,\n    usecols,\n    dtype,\n    engine,\n    converters,\n    true_values,\n    false_values,\n    skiprows,\n    nrows,\n    na_values,\n    keep_default_na,\n    na_filter,\n    verbose,\n    parse_dates,\n    date_parser,\n    date_format,\n    thousands,\n    decimal,\n    comment,\n    skipfooter,\n    storage_options,\n    dtype_backend,\n    engine_kwargs\n)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    503\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m     )\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_default_na\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_default_na\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_filter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_filter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecimal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;66;03m# make sure to close opened file handles\u001b[39;00m\n\u001b[1;32m    535\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m should_close:\n",
      "File \u001b[0;32m~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/io/excel/_base.py:1616\u001b[0m, in \u001b[0;36mExcelFile.parse\u001b[0;34m(\n    self,\n    sheet_name,\n    header,\n    names,\n    index_col,\n    usecols,\n    converters,\n    true_values,\n    false_values,\n    skiprows,\n    nrows,\n    na_values,\n    parse_dates,\n    date_parser,\n    date_format,\n    thousands,\n    comment,\n    skipfooter,\n    dtype_backend,\n    **kwds\n)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\n\u001b[1;32m   1577\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1578\u001b[0m     sheet_name: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds,\n\u001b[1;32m   1597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, DataFrame] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mint\u001b[39m, DataFrame]:\n\u001b[1;32m   1598\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m \u001b[38;5;124;03m    Parse specified sheet(s) into a DataFrame.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1614\u001b[0m \u001b[38;5;124;03m    >>> file.parse()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1615\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1616\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1617\u001b[0m \u001b[43m        \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[43m        \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrue_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfalse_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfalse_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnrows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_parser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthousands\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthousands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipfooter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipfooter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/io/excel/_base.py:778\u001b[0m, in \u001b[0;36mBaseExcelReader.parse\u001b[0;34m(\n    self,\n    sheet_name,\n    header,\n    names,\n    index_col,\n    usecols,\n    dtype,\n    true_values,\n    false_values,\n    skiprows,\n    nrows,\n    na_values,\n    verbose,\n    parse_dates,\n    date_parser,\n    date_format,\n    thousands,\n    decimal,\n    comment,\n    skipfooter,\n    dtype_backend,\n    **kwds\n)\u001b[0m\n\u001b[1;32m    775\u001b[0m     sheet \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_sheet_by_index(asheetname)\n\u001b[1;32m    777\u001b[0m file_rows_needed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calc_rows(header, index_col, skiprows, nrows)\n\u001b[0;32m--> 778\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_sheet_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msheet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_rows_needed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(sheet, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;66;03m# pyxlsb opens two TemporaryFiles\u001b[39;00m\n\u001b[1;32m    781\u001b[0m     sheet\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/io/excel/_openpyxl.py:615\u001b[0m, in \u001b[0;36mOpenpyxlReader.get_sheet_data\u001b[0;34m(self, sheet, file_rows_needed)\u001b[0m\n\u001b[1;32m    613\u001b[0m data: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[Scalar]] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    614\u001b[0m last_row_with_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 615\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row_number, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sheet\u001b[38;5;241m.\u001b[39mrows):\n\u001b[1;32m    616\u001b[0m     converted_row \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_cell(cell) \u001b[38;5;28;01mfor\u001b[39;00m cell \u001b[38;5;129;01min\u001b[39;00m row]\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m converted_row \u001b[38;5;129;01mand\u001b[39;00m converted_row[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;66;03m# trim trailing empty elements\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/pytorch_m1/lib/python3.9/site-packages/openpyxl/worksheet/_read_only.py:92\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._cells_by_row\u001b[0;34m(\n    self,\n    min_col,\n    min_row,\n    max_col,\n    max_row,\n    values_only\n)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;66;03m# return cells from a row\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m idx:\n\u001b[0;32m---> 92\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_col\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m row\n",
      "File \u001b[0;32m~/Desktop/pytorch_m1/lib/python3.9/site-packages/openpyxl/worksheet/_read_only.py:123\u001b[0m, in \u001b[0;36mReadOnlyWorksheet._get_row\u001b[0;34m(\n    self,\n    row,\n    min_col,\n    max_col,\n    values_only\n)\u001b[0m\n\u001b[1;32m    121\u001b[0m         new_row[idx] \u001b[38;5;241m=\u001b[39m cell[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values_only:\n\u001b[0;32m--> 123\u001b[0m             \u001b[43mnew_row\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m ReadOnlyCell(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcell)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_row)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the datasets\n",
    "df2 = pd.read_excel(\"final_dataset_with_enhanced_topics_2.xlsx\")\n",
    "df = pd.read_excel(\"KCDataSet.xlsx\")\n",
    "print(df.columns)\n",
    "\n",
    "# Select only the desired columns in df\n",
    "df = df[['Case Number', 'Customer ID', 'Mailing State/Province', 'Contact: Email', 'Date/Time Opened']]\n",
    "\n",
    "# Merge the two dataframes on 'Case Number'\n",
    "combined_df = pd.merge(df2, df, on='Case Number', how='inner')\n",
    "combined_df.rename(columns={'Mailing State/Province': 'State'}, inplace=True)\n",
    "combined_df.rename(columns={'Contact: Email': 'Email'}, inplace=True)\n",
    "combined_df.rename(columns={'Date/Time Opened': 'Date'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_excel(\"Tableau_Ready_Check_States_50_10.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizations\n",
    "print(combined_df.rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(327377, 5)\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.read_excel(\"Tableau_Ready_Check_States_50_10.xlsx\")\n",
    "df3 = pd.read_excel(\"no_dates_industry_plus_states.xlsx\")\n",
    "print(df3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Case Number1', 'Email1', 'Billaddr State', 'Industry Category',\n",
      "       'GPT Improved Labels'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df3.columns)\n",
    "# need to add Date to this and then start plot\n",
    "#  join with combined_df + df3, extract date column from combined_df\n",
    "#create final df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(326716, 6)\n",
      "Index(['Case Number1', 'Email1', 'Billaddr State', 'Industry Category',\n",
      "       'GPT Improved Labels', 'Date'],\n",
      "      dtype='object')\n",
      "Index(['Case Number', 'Email', 'State', 'Industry Category',\n",
      "       'GPT Improved Labels', 'Date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming combined_df and df3 are already defined\n",
    "# Perform the merge operation using an inner join\n",
    "df_final = pd.merge(left=df3, \n",
    "                    right=combined_df[['Case Number', 'Date']], \n",
    "                    left_on='Case Number1', \n",
    "                    right_on='Case Number', \n",
    "                    how='inner')  # Change to 'inner' to ensure only matching records are included\n",
    "\n",
    "# Optionally, if you don't need the 'Case Number' from combined_df in the final dataframe:\n",
    "df_final.drop(columns=['Case Number'], inplace=True)\n",
    "\n",
    "# Now df_final should have all columns from df3 and the 'Date' column from combined_df\n",
    "print(df_final.shape)\n",
    "print(df_final.columns)\n",
    "\n",
    "# Rename columns to unify names across the final DataFrame\n",
    "df_final.rename(columns={'Billaddr State': 'State'}, inplace=True)\n",
    "df_final.rename(columns={'Email1': 'Email'}, inplace=True)\n",
    "df_final.rename(columns={'Case Number1': 'Case Number'}, inplace=True)\n",
    "\n",
    "# Print updated columns to confirm changes\n",
    "print(df_final.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "State Counts:\n",
      "State\n",
      "CA                     46145\n",
      "TX                     21735\n",
      "FL                     18842\n",
      "NY                     18576\n",
      "IL                     15498\n",
      "                       ...  \n",
      "LEMESOS                    1\n",
      "CENTRAL                    1\n",
      "Trinidad and Tobago        1\n",
      "Java                       1\n",
      "Leics.                     1\n",
      "Name: count, Length: 339, dtype: int64\n",
      "\n",
      "DataFrame After Processing:\n",
      "        Case Number                           Email        State  \\\n",
      "0            1202.0                  cyee@bayem.org           MO   \n",
      "1            1207.0          kelly@rrspecialist.net           IN   \n",
      "2            1272.0       colleennicoll45@gmail.com  Maharashtra   \n",
      "3            1279.0      nellieh@allstatefloral.com          NaN   \n",
      "4            1297.0           l.morain@gymshark.com           PA   \n",
      "...             ...                             ...          ...   \n",
      "326711    1761124.0   rhonda.west@ardexamericas.com           CA   \n",
      "326712    1761131.0      kate@spacecoasthabitat.org           VA   \n",
      "326713    1761142.0  admin@apexheartandvascular.com           WI   \n",
      "326714    1761144.0       kellilynnhall16@gmail.com           TN   \n",
      "326715    1761154.0         meganhmartins@gmail.com           MD   \n",
      "\n",
      "              Industry Category  \\\n",
      "0                    Consulting   \n",
      "1                         Other   \n",
      "2                   Real Estate   \n",
      "3                      Services   \n",
      "4                           NaN   \n",
      "...                         ...   \n",
      "326711                      NaN   \n",
      "326712   Automotive & Transport   \n",
      "326713  Transportation Services   \n",
      "326714                      NaN   \n",
      "326715                      NaN   \n",
      "\n",
      "                                      GPT Improved Labels                Date  \n",
      "0                \"Employee PTO Request for Paid Time Off\" 2019-04-22 22:36:00  \n",
      "1       \"Employment Assistance for Non-Profit Organiza... 2019-04-22 22:45:00  \n",
      "2       \"Expert Recruitment Services for Improving Com... 2019-04-23 02:23:00  \n",
      "3       \"Employee Handbook Policies on Social Media, D... 2019-04-23 03:04:00  \n",
      "4                         \"Job Search Assistance Inquiry\" 2019-04-23 07:33:00  \n",
      "...                                                   ...                 ...  \n",
      "326711                                 FMLA Leave Request 2024-03-26 11:47:00  \n",
      "326712  \"State Paid Leave Laws for Multi-State Employe... 2024-03-26 11:50:00  \n",
      "326713  \"Employee Leave and COVID-19 Forms and Regulat... 2024-03-26 11:53:00  \n",
      "326714         \"Employee Religious Accommodation Request\" 2024-03-26 11:53:00  \n",
      "326715             Employee Medical Accommodation Request 2024-03-26 11:57:00  \n",
      "\n",
      "[326716 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Replace 'Unknown Date' with '2000-01-01' or any specific date you consider as a placeholder\n",
    "df_final['Date'] = pd.to_datetime(df_final['Date'].fillna('2000-01-01'), errors='coerce')\n",
    "\n",
    "# Remove any duplicate rows, if necessary\n",
    "df_final.drop_duplicates(inplace=True)\n",
    "\n",
    "# Sort the DataFrame by 'Date' without setting it as the index\n",
    "df_final.sort_values('Date', inplace=True)\n",
    "\n",
    "# Example: Count the number of cases by state\n",
    "print(\"\\nState Counts:\")\n",
    "state_counts = df_final['State'].value_counts()\n",
    "print(state_counts)\n",
    "\n",
    "# Display the DataFrame columns and the first few rows to ensure 'Date' is still present\n",
    "print(\"\\nDataFrame After Processing:\")\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Case Number', 'Email', 'State', 'Industry Category',\n",
      "       'GPT Improved Labels', 'Date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_final.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Case Number                           Email        Industry Category  \\\n",
      "0            1202.0                  cyee@bayem.org               Consulting   \n",
      "1            1207.0          kelly@rrspecialist.net                    Other   \n",
      "2            1272.0       colleennicoll45@gmail.com              Real Estate   \n",
      "4            1297.0           l.morain@gymshark.com                      NaN   \n",
      "5            1323.0          agnes.tham@laverne.edu                Insurance   \n",
      "...             ...                             ...                      ...   \n",
      "326711    1761124.0   rhonda.west@ardexamericas.com                      NaN   \n",
      "326712    1761131.0      kate@spacecoasthabitat.org   Automotive & Transport   \n",
      "326713    1761142.0  admin@apexheartandvascular.com  Transportation Services   \n",
      "326714    1761144.0       kellilynnhall16@gmail.com                      NaN   \n",
      "326715    1761154.0         meganhmartins@gmail.com                      NaN   \n",
      "\n",
      "                                      GPT Improved Labels                Date  \\\n",
      "0                \"Employee PTO Request for Paid Time Off\" 2019-04-22 22:36:00   \n",
      "1       \"Employment Assistance for Non-Profit Organiza... 2019-04-22 22:45:00   \n",
      "2       \"Expert Recruitment Services for Improving Com... 2019-04-23 02:23:00   \n",
      "4                         \"Job Search Assistance Inquiry\" 2019-04-23 07:33:00   \n",
      "5       \"Human Resource Management Training Program fo... 2019-04-23 08:18:00   \n",
      "...                                                   ...                 ...   \n",
      "326711                                 FMLA Leave Request 2024-03-26 11:47:00   \n",
      "326712  \"State Paid Leave Laws for Multi-State Employe... 2024-03-26 11:50:00   \n",
      "326713  \"Employee Leave and COVID-19 Forms and Regulat... 2024-03-26 11:53:00   \n",
      "326714         \"Employee Religious Accommodation Request\" 2024-03-26 11:53:00   \n",
      "326715             Employee Medical Accommodation Request 2024-03-26 11:57:00   \n",
      "\n",
      "                     State  \n",
      "0                 Missouri  \n",
      "1                       IN  \n",
      "2       Maharashtra, India  \n",
      "4             Pennsylvania  \n",
      "5             Pennsylvania  \n",
      "...                    ...  \n",
      "326711          California  \n",
      "326712            Virginia  \n",
      "326713           Wisconsin  \n",
      "326714           Tennessee  \n",
      "326715            Maryland  \n",
      "\n",
      "[311948 rows x 6 columns]\n",
      "Data normalization completed and the DataFrame is now clean.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Normalization dictionary to map incorrect or varied state names to a standard form\n",
    "normalization_dict = {\n",
    "    # U.S. States\n",
    "    \"MO\": \"Missouri\", \"FL\": \"Florida\", \"CA\": \"California\", \"TX\": \"Texas\", \"NY\": \"New York\", \"VA\": \"Virginia\",\n",
    "    \"PA\": \"Pennsylvania\", \"KY\": \"Kentucky\", \"NJ\": \"New Jersey\", \"NC\": \"North Carolina\", \"OR\": \"Oregon\",\n",
    "    \"MN\": \"Minnesota\", \"OK\": \"Oklahoma\", \"WI\": \"Wisconsin\", \"MD\": \"Maryland\", \"DC\": \"Washington D.C.\",\n",
    "    \"IL\": \"Illinois\", \"CO\": \"Colorado\", \"CT\": \"Connecticut\", \"NV\": \"Nevada\", \"SC\": \"South Carolina\",\n",
    "    \"KS\": \"Kansas\", \"MI\": \"Michigan\", \"GA\": \"Georgia\", \"OH\": \"Ohio\", \"AZ\": \"Arizona\", \"AL\": \"Alabama\",\n",
    "    \"LA\": \"Louisiana\", \"WA\": \"Washington\", \"VT\": \"Vermont\", \"TN\": \"Tennessee\", \"IA\": \"Iowa\", \"AK\": \"Alaska\",\n",
    "    \"NM\": \"New Mexico\", \"NH\": \"New Hampshire\", \"MS\": \"Mississippi\", \"ME\": \"Maine\", \"AR\": \"Arkansas\",\n",
    "    \"UT\": \"Utah\", \"NE\": \"Nebraska\", \"DE\": \"Delaware\", \"ID\": \"Idaho\", \"MT\": \"Montana\", \"SD\": \"South Dakota\",\n",
    "    \"ND\": \"North Dakota\", \"HI\": \"Hawaii\", \"WV\": \"West Virginia\", \"RI\": \"Rhode Island\", \"PR\": \"Puerto Rico\",\n",
    "    \"GU\": \"Guam\", \"VI\": \"U.S. Virgin Islands\", \"BVI\": \"British Virgin Islands\",\n",
    "\n",
    "    # International names and corrections\n",
    "    \"Maharashtra\": \"Maharashtra, India\", \"Dublin\": \"Dublin, Ireland\", \"Haryana\": \"Haryana, India\",\n",
    "    \"Tamil Nadu\": \"Tamil Nadu, India\", \"Delhi\": \"Delhi, India\", \"Uttar Pradesh\": \"Uttar Pradesh, India\",\n",
    "    \"Punjab\": \"Punjab, India\", \"Karnataka\": \"Karnataka, India\", \"Gujarat\": \"Gujarat, India\",\n",
    "    \"Telangana\": \"Telangana, India\", \"Madhya Pradesh\": \"Madhya Pradesh, India\", \"Orissa\": \"Odisha, India\",\n",
    "    \"Sindh\": \"Sindh, Pakistan\", \"Riyadh\": \"Riyadh, Saudi Arabia\", \"Jamaica\": \"Jamaica\",\n",
    "    \"Kenya\": \"Kenya\", \"Singapore\": \"Singapore\", \"Dubai\": \"Dubai, UAE\", \"Lebanon\": \"Lebanon\",\n",
    "    \"Kuwait\": \"Kuwait\", \"Qatar\": \"Qatar\", \"Bahamas\": \"Bahamas\", \"Panama\": \"Panama\",\n",
    "    \"London\": \"London, UK\", \"Berlin\": \"Berlin, Germany\", \"Paris\": \"Paris, France\",\n",
    "    \"Istanbul\": \"Istanbul, Turkey\", \"Moscow\": \"Moscow, Russia\", \"Tokyo\": \"Tokyo, Japan\",\n",
    "    \"Beirut\": \"Beirut, Lebanon\", \"São Paulo\": \"São Paulo, Brazil\", \"Buenos Aires\": \"Buenos Aires, Argentina\",\n",
    "    \"Kingston\": \"Kingston, Jamaica\", \"Eastern Province\": \"Eastern Province, Saudi Arabia\",\n",
    "    \"Cairo\": \"Cairo, Egypt\", \"Nairobi\": \"Nairobi, Kenya\", \"Auckland\": \"Auckland, New Zealand\",\n",
    "    \"Sydney\": \"Sydney, Australia\", \"Melbourne\": \"Melbourne, Australia\", \"Queensland\": \"Queensland, Australia\",\n",
    "    \"New South Wales\": \"New South Wales, Australia\", \"Abu Dhabi\": \"Abu Dhabi, UAE\",\n",
    "    \"New Delhi\": \"New Delhi, India\",\n",
    "\n",
    "    # Common errors and placeholders\n",
    "    \"-Select State-\": None, \"Unknown\": None, \"None/Other\": None, \"----\": None,\n",
    "    \"No states available\": None, \"[Please Select]\": None, \"Not applicable\": None,\n",
    "    \"Other/Not Applicable\": None, \"DC_x0005_DC\": \"Washington D.C.\", \"35\": None, \"7839\": None,\n",
    "\n",
    "    # Specific city or region corrections\n",
    "    \"愛知県\": \"Aichi, Japan\", \"東京都\": \"Tokyo, Japan\", \"New Providence\": \"New Providence, Bahamas\",\n",
    "    \"Makkah\": \"Makkah, Saudi Arabia\", \"Greater Accra\": \"Greater Accra, Ghana\", \"Eastern\": \"Eastern Region\",\n",
    "    \"Central\": \"Central Region\", \"Western\": \"Western Region\", \"Hamilton\": \"Hamilton, Bermuda\",\n",
    "\n",
    "    # Add all other mappings here as necessary\n",
    "}\n",
    "\n",
    "# Normalize the State column\n",
    "df_final['Normalized State'] = df_final['State'].apply(lambda x: normalization_dict.get(x, x))\n",
    "\n",
    "# Optionally, remove rows where the state could not be normalized (if None is unacceptable)\n",
    "df_final = df_final[df_final['Normalized State'].notna()]\n",
    "\n",
    "# Remove the original 'State' column\n",
    "df_final.drop('State', axis=1, inplace=True)\n",
    "\n",
    "# Rename 'Normalized State' to 'State'\n",
    "df_final.rename(columns={'Normalized State': 'State'}, inplace=True)\n",
    "\n",
    "# Print the updated DataFrame to verify changes\n",
    "print(df_final)\n",
    "\n",
    "# Optionally, save the cleaned DataFrame to a new CSV file\n",
    "# df_final.to_csv('cleaned_states.csv', index=False)\n",
    "\n",
    "print(\"Data normalization completed and the DataFrame is now clean.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Case Number', 'Email', 'Industry Category', 'GPT Improved Labels',\n",
      "       'Date', 'State'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Data:\n",
      "   Case Number                      Email Industry Category  \\\n",
      "0       1202.0             cyee@bayem.org        Consulting   \n",
      "1       1207.0     kelly@rrspecialist.net             Other   \n",
      "2       1272.0  colleennicoll45@gmail.com       Real Estate   \n",
      "4       1297.0      l.morain@gymshark.com               NaN   \n",
      "5       1323.0     agnes.tham@laverne.edu         Insurance   \n",
      "\n",
      "                                 GPT Improved Labels                Date  \\\n",
      "0           \"Employee PTO Request for Paid Time Off\" 2019-04-22 22:36:00   \n",
      "1  \"Employment Assistance for Non-Profit Organiza... 2019-04-22 22:45:00   \n",
      "2  \"Expert Recruitment Services for Improving Com... 2019-04-23 02:23:00   \n",
      "4                    \"Job Search Assistance Inquiry\" 2019-04-23 07:33:00   \n",
      "5  \"Human Resource Management Training Program fo... 2019-04-23 08:18:00   \n",
      "\n",
      "                State  \n",
      "0            Missouri  \n",
      "1                  IN  \n",
      "2  Maharashtra, India  \n",
      "4        Pennsylvania  \n",
      "5        Pennsylvania  \n",
      "\n",
      "Null Values Check:\n",
      "Case Number: 0 null values\n",
      "Email: 4797 null values\n",
      "Industry Category: 101976 null values\n",
      "GPT Improved Labels: 17 null values\n",
      "Date: 0 null values\n",
      "State: 0 null values\n"
     ]
    }
   ],
   "source": [
    "# Display the first few rows of the DataFrame\n",
    "print(\"Initial Data:\")\n",
    "print(df_final.head())\n",
    "\n",
    "# Checking for null values in each column\n",
    "print(\"\\nNull Values Check:\")\n",
    "for column in df_final.columns:\n",
    "    # Count the number of null values in each column\n",
    "    null_count = df_final[column].isnull().sum()\n",
    "    # Print the column name and the number of null values\n",
    "    print(f\"{column}: {null_count} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated DataFrame shape after removing NULL in 'GPT Improved Labels': (311931, 6)\n",
      "        Case Number                           Email        Industry Category  \\\n",
      "0            1202.0                  cyee@bayem.org               Consulting   \n",
      "1            1207.0          kelly@rrspecialist.net                    Other   \n",
      "2            1272.0       colleennicoll45@gmail.com              Real Estate   \n",
      "4            1297.0           l.morain@gymshark.com                      NaN   \n",
      "5            1323.0          agnes.tham@laverne.edu                Insurance   \n",
      "...             ...                             ...                      ...   \n",
      "326711    1761124.0   rhonda.west@ardexamericas.com                      NaN   \n",
      "326712    1761131.0      kate@spacecoasthabitat.org   Automotive & Transport   \n",
      "326713    1761142.0  admin@apexheartandvascular.com  Transportation Services   \n",
      "326714    1761144.0       kellilynnhall16@gmail.com                      NaN   \n",
      "326715    1761154.0         meganhmartins@gmail.com                      NaN   \n",
      "\n",
      "                                      GPT Improved Labels                Date  \\\n",
      "0                \"Employee PTO Request for Paid Time Off\" 2019-04-22 22:36:00   \n",
      "1       \"Employment Assistance for Non-Profit Organiza... 2019-04-22 22:45:00   \n",
      "2       \"Expert Recruitment Services for Improving Com... 2019-04-23 02:23:00   \n",
      "4                         \"Job Search Assistance Inquiry\" 2019-04-23 07:33:00   \n",
      "5       \"Human Resource Management Training Program fo... 2019-04-23 08:18:00   \n",
      "...                                                   ...                 ...   \n",
      "326711                                 FMLA Leave Request 2024-03-26 11:47:00   \n",
      "326712  \"State Paid Leave Laws for Multi-State Employe... 2024-03-26 11:50:00   \n",
      "326713  \"Employee Leave and COVID-19 Forms and Regulat... 2024-03-26 11:53:00   \n",
      "326714         \"Employee Religious Accommodation Request\" 2024-03-26 11:53:00   \n",
      "326715             Employee Medical Accommodation Request 2024-03-26 11:57:00   \n",
      "\n",
      "                     State  \n",
      "0                 Missouri  \n",
      "1                       IN  \n",
      "2       Maharashtra, India  \n",
      "4             Pennsylvania  \n",
      "5             Pennsylvania  \n",
      "...                    ...  \n",
      "326711          California  \n",
      "326712            Virginia  \n",
      "326713           Wisconsin  \n",
      "326714           Tennessee  \n",
      "326715            Maryland  \n",
      "\n",
      "[311931 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where 'GPT Improved Labels' is NULL\n",
    "df_final = df_final.dropna(subset=['GPT Improved Labels'])\n",
    "\n",
    "# Print the new DataFrame shape to see the effect of the row removal\n",
    "print(\"Updated DataFrame shape after removing NULL in 'GPT Improved Labels':\", df_final.shape)\n",
    "print(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Case Number', 'Email', 'Industry Category', 'GPT Improved Labels',\n",
       "       'Date', 'State'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_final is finally ready\n",
    "df_final.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499\n"
     ]
    }
   ],
   "source": [
    "print(len(df_final['GPT Improved Labels'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_32477/4192741321.py:10: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "/var/folders/p6/lz9nv0md22z21j6w4rb19b380000gp/T/ipykernel_32477/4192741321.py:13: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataset\n",
    "df = df_final\n",
    "\n",
    "# Drop rows where 'Email' is null if these records are not useful\n",
    "df = df[df['Email'].notna()]\n",
    "\n",
    "# Fill missing 'Industry Category' with a placeholder if necessary\n",
    "df['Industry Category'] = df['Industry Category'].fillna('Unknown')\n",
    "\n",
    "# Ensure proper data types\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x336b2efa0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocess data\n",
    "df_final.dropna(subset=['Email'], inplace=True)\n",
    "df_final.dropna(subset=['Industry Category'], inplace=True)\n",
    "\n",
    "# Define Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define topics\n",
    "topics = df_final['GPT Improved Labels'].unique()\n",
    "\n",
    "# Define layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Interactive Dashboard\"),\n",
    "    dcc.Dropdown(\n",
    "        id='topic-dropdown',\n",
    "        options=[{'label': topic, 'value': topic} for topic in topics],\n",
    "        value=topics[0]  # default value\n",
    "    ),\n",
    "    html.Div(dcc.Graph(id='topic-industry-state-graph'), style={'width': '95%', 'margin': 'auto'})\n",
    "])\n",
    "\n",
    "# Define callback to update graph based on dropdown selection\n",
    "@app.callback(\n",
    "    Output('topic-industry-state-graph', 'figure'),\n",
    "    [Input('topic-dropdown', 'value')]\n",
    ")\n",
    "def update_graph(selected_topic):\n",
    "    filtered_df = df_final[df_final['GPT Improved Labels'] == selected_topic]\n",
    "\n",
    "    # Count occurrences of each industry category\n",
    "    industry_counts = filtered_df['Industry Category'].value_counts()\n",
    "\n",
    "    # Count occurrences of each state for each industry category\n",
    "    state_counts = {}\n",
    "    for industry_category in industry_counts.index:\n",
    "        state_counts[industry_category] = filtered_df[filtered_df['Industry Category'] == industry_category]['State'].value_counts()\n",
    "\n",
    "    # Create scatter plot traces for each industry category\n",
    "    traces = []\n",
    "    for industry_category, count in industry_counts.items():\n",
    "        trace = go.Scatter3d(\n",
    "            x=[industry_category] * len(state_counts[industry_category]),\n",
    "            y=list(state_counts[industry_category].index),\n",
    "            z=list(state_counts[industry_category].values),\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                symbol='circle',\n",
    "                line=dict(width=1)\n",
    "            ),\n",
    "            name=f'{industry_category}'\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    return {\n",
    "        'data': traces,\n",
    "        'layout': go.Layout(\n",
    "            title=f'Distribution of Industry Category and State for {selected_topic}',\n",
    "            scene=dict(\n",
    "                xaxis=dict(title='Industry Category'),\n",
    "                yaxis=dict(title='State'),\n",
    "                zaxis=dict(title='Frequency')\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Run app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x43d85baf0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocess data\n",
    "df_final.dropna(subset=['Email'], inplace=True)\n",
    "df_final.dropna(subset=['Industry Category'], inplace=True)\n",
    "\n",
    "# Define Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define topics\n",
    "topics = df_final['GPT Improved Labels'].unique()\n",
    "\n",
    "# Define layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Interactive Dashboard\"),\n",
    "    dcc.Dropdown(\n",
    "        id='topic-dropdown',\n",
    "        options=[{'label': topic, 'value': topic} for topic in topics],\n",
    "        value=topics[0]  # default value\n",
    "    ),\n",
    "    html.Div(\n",
    "        dcc.Graph(id='topic-industry-state-graph'), \n",
    "        style={'width': '95%', 'margin': 'auto', 'padding': '50px'}\n",
    "    )\n",
    "])\n",
    "\n",
    "# Define callback to update graph based on dropdown selection\n",
    "@app.callback(\n",
    "    Output('topic-industry-state-graph', 'figure'),\n",
    "    [Input('topic-dropdown', 'value')]\n",
    ")\n",
    "def update_graph(selected_topic):\n",
    "    filtered_df = df_final[df_final['GPT Improved Labels'] == selected_topic]\n",
    "\n",
    "    # Count occurrences of each industry category\n",
    "    industry_counts = filtered_df['Industry Category'].value_counts()\n",
    "\n",
    "    # Count occurrences of each state for each industry category\n",
    "    state_counts = {}\n",
    "    for industry_category in industry_counts.index:\n",
    "        state_counts[industry_category] = filtered_df[filtered_df['Industry Category'] == industry_category]['State'].value_counts()\n",
    "\n",
    "    # Create scatter plot traces for each industry category\n",
    "    traces = []\n",
    "    for industry_category, count in industry_counts.items():\n",
    "        trace = go.Scatter3d(\n",
    "            x=[industry_category] * len(state_counts[industry_category]),\n",
    "            y=list(state_counts[industry_category].index),\n",
    "            z=list(state_counts[industry_category].values),\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=8,\n",
    "                symbol='circle',\n",
    "                line=dict(width=1)\n",
    "            ),\n",
    "            name=f'{industry_category}'\n",
    "        )\n",
    "        traces.append(trace)\n",
    "\n",
    "    return {\n",
    "        'data': traces,\n",
    "        'layout': go.Layout(\n",
    "            title=f'Distribution of Industry Category and State for {selected_topic}',\n",
    "            scene=dict(\n",
    "                xaxis=dict(title='Industry Category'),\n",
    "                yaxis=dict(title='State'),\n",
    "                zaxis=dict(title='Frequency')\n",
    "            )\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Run app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x48b5c2700>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocess data\n",
    "df_final.dropna(subset=['Email'], inplace=True)\n",
    "df_final.dropna(subset=['Industry Category'], inplace=True)\n",
    "\n",
    "# Define Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define topics\n",
    "topics = df_final['GPT Improved Labels'].unique()\n",
    "\n",
    "# Define layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Interactive Dashboard\"),\n",
    "    html.Div([\n",
    "        html.Div([\n",
    "            html.H2(\"Topic vs State\"),\n",
    "            dcc.Graph(id='topic-state-graph', style={'width': '100%', 'height': '60vh'})\n",
    "        ], style={'width': '50%', 'display': 'inline-block'}),\n",
    "        html.Div([\n",
    "            html.H2(\"Topic vs Industry Category\"),\n",
    "            dcc.Graph(id='topic-industry-graph', style={'width': '100%', 'height': '60vh'})\n",
    "        ], style={'width': '50%', 'display': 'inline-block'})\n",
    "    ]),\n",
    "    html.Div([\n",
    "        html.H3(\"Select Topic for State View:\"),\n",
    "        dcc.Dropdown(\n",
    "            id='state-topic-dropdown',\n",
    "            options=[{'label': topic, 'value': topic} for topic in topics],\n",
    "            value=topics[0]  # default value\n",
    "        )\n",
    "    ]),\n",
    "    html.Div([\n",
    "        html.H3(\"Select Topic for Industry Category View:\"),\n",
    "        dcc.Dropdown(\n",
    "            id='industry-topic-dropdown',\n",
    "            options=[{'label': topic, 'value': topic} for topic in topics],\n",
    "            value=topics[0]  # default value\n",
    "        )\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Define callback to update state graph based on dropdown selection\n",
    "@app.callback(\n",
    "    Output('topic-state-graph', 'figure'),\n",
    "    [Input('state-topic-dropdown', 'value')]\n",
    ")\n",
    "def update_state_graph(selected_topic):\n",
    "    filtered_df = df_final[df_final['GPT Improved Labels'] == selected_topic]\n",
    "\n",
    "    # Count occurrences of each state\n",
    "    state_counts = filtered_df['State'].value_counts()\n",
    "\n",
    "    # Create bar plot trace for State distribution\n",
    "    state_trace = go.Bar(\n",
    "        x=state_counts.index,\n",
    "        y=state_counts.values,\n",
    "        name='State'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'data': [state_trace],\n",
    "        'layout': go.Layout(\n",
    "            title=f'Distribution of State for {selected_topic}',\n",
    "            xaxis=dict(title='State'),\n",
    "            yaxis=dict(title='Frequency')\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Define callback to update industry graph based on dropdown selection\n",
    "@app.callback(\n",
    "    Output('topic-industry-graph', 'figure'),\n",
    "    [Input('industry-topic-dropdown', 'value')]\n",
    ")\n",
    "def update_industry_graph(selected_topic):\n",
    "    filtered_df = df_final[df_final['GPT Improved Labels'] == selected_topic]\n",
    "\n",
    "    # Count occurrences of each industry category\n",
    "    industry_counts = filtered_df['Industry Category'].value_counts()\n",
    "\n",
    "    # Create bar plot trace for Industry Category distribution\n",
    "    industry_trace = go.Bar(\n",
    "        x=industry_counts.index,\n",
    "        y=industry_counts.values,\n",
    "        name='Industry Category'\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        'data': [industry_trace],\n",
    "        'layout': go.Layout(\n",
    "            title=f'Distribution of Industry Category for {selected_topic}',\n",
    "            xaxis=dict(title='Industry Category'),\n",
    "            yaxis=dict(title='Frequency')\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Run app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x43d2336a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(\n",
      "    self=Index(['AB', 'AP', 'AS',\n",
      "       'AZ_x0005_AZ_x00...,\n",
      "      dtype='object', name='State', length=172),\n",
      "    key='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "   3804 try:\n",
      "-> 3805     return self._engine.get_loc(casted_key)\n",
      "        casted_key = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        self = Index(['AB', 'AP', 'AS',\n",
      "       'AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ',\n",
      "       'Abu Dhabi, UAE', 'Al', 'Alabama', 'Alaska',\n",
      "       'Andaman and Nicobar Islands', 'Antigua & Barbuda',\n",
      "       ...\n",
      "       'W Yorkshire', 'WY', 'Washington', 'Washington D.C.',\n",
      "       'West Coast Demerara', 'West Virginia', 'Wexford', 'Wisconsin', 'Worcs',\n",
      "       '北京市'],\n",
      "      dtype='object', name='State', length=172)\n",
      "   3806 except KeyError as err:\n",
      "\n",
      "File index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
      "\n",
      "File index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
      "\n",
      "File pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
      "\n",
      "File pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
      "\n",
      "KeyError: '\"Employee PTO Request for Paid Time Off\"'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "Cell In[96], line 48, in update_state_bar_chart(\n",
      "    selected_topic='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "     44 # Create bar chart traces for the selected topic and top states\n",
      "     45 traces = []\n",
      "     46 traces.append(go.Bar(\n",
      "     47     x=[selected_topic],\n",
      "---> 48     y=[topic_data[selected_topic]],\n",
      "        traces = []\n",
      "        go = <module 'plotly.graph_objs' from '/Users/muzammil.mahmud/Desktop/pytorch_m1/lib/python3.9/site-packages/plotly/graph_objs/__init__.py'>\n",
      "        selected_topic = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        [selected_topic] = ['\"Employee PTO Request for Paid Time Off\"']\n",
      "        topic_data = State\n",
      "AB                                                          0\n",
      "AP                                                          0\n",
      "AS                                                          0\n",
      "AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ    0\n",
      "Abu Dhabi, UAE                                              0\n",
      "                                                           ..\n",
      "West Virginia                                               0\n",
      "Wexford                                                     0\n",
      "Wisconsin                                                   8\n",
      "Worcs                                                       0\n",
      "北京市                                                         0\n",
      "Name: \"Employee PTO Request for Paid Time Off\", Length: 172, dtype: int64\n",
      "     49     name=selected_topic\n",
      "     50 ))\n",
      "     52 for state in top_states.index:\n",
      "     53     traces.append(go.Bar(\n",
      "     54         x=[selected_topic],\n",
      "     55         y=[topic_data[state]],\n",
      "     56         name=state,\n",
      "     57         showlegend=False\n",
      "     58     ))\n",
      "\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/series.py:1112, in Series.__getitem__(\n",
      "    self=State\n",
      "AB                                        ...est for Paid Time Off\", Length: 172, dtype: int64,\n",
      "    key='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "   1109     return self._values[key]\n",
      "   1111 elif key_is_scalar:\n",
      "-> 1112     return self._get_value(key)\n",
      "        key = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        self = State\n",
      "AB                                                          0\n",
      "AP                                                          0\n",
      "AS                                                          0\n",
      "AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ    0\n",
      "Abu Dhabi, UAE                                              0\n",
      "                                                           ..\n",
      "West Virginia                                               0\n",
      "Wexford                                                     0\n",
      "Wisconsin                                                   8\n",
      "Worcs                                                       0\n",
      "北京市                                                         0\n",
      "Name: \"Employee PTO Request for Paid Time Off\", Length: 172, dtype: int64\n",
      "   1114 # Convert generator to list before going through hashable part\n",
      "   1115 # (We will iterate through the generator there to check for slices)\n",
      "   1116 if is_iterator(key):\n",
      "\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/series.py:1228, in Series._get_value(\n",
      "    self=State\n",
      "AB                                        ...est for Paid Time Off\", Length: 172, dtype: int64,\n",
      "    label='\"Employee PTO Request for Paid Time Off\"',\n",
      "    takeable=False\n",
      ")\n",
      "   1225     return self._values[label]\n",
      "   1227 # Similar to Index.get_value, but we do not fall back to positional\n",
      "-> 1228 loc = self.index.get_loc(label)\n",
      "        label = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        self = State\n",
      "AB                                                          0\n",
      "AP                                                          0\n",
      "AS                                                          0\n",
      "AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ    0\n",
      "Abu Dhabi, UAE                                              0\n",
      "                                                           ..\n",
      "West Virginia                                               0\n",
      "Wexford                                                     0\n",
      "Wisconsin                                                   8\n",
      "Worcs                                                       0\n",
      "北京市                                                         0\n",
      "Name: \"Employee PTO Request for Paid Time Off\", Length: 172, dtype: int64\n",
      "   1230 if is_integer(loc):\n",
      "   1231     return self._values[loc]\n",
      "\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(\n",
      "    self=Index(['AB', 'AP', 'AS',\n",
      "       'AZ_x0005_AZ_x00...,\n",
      "      dtype='object', name='State', length=172),\n",
      "    key='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "   3807     if isinstance(casted_key, slice) or (\n",
      "   3808         isinstance(casted_key, abc.Iterable)\n",
      "   3809         and any(isinstance(x, slice) for x in casted_key)\n",
      "   3810     ):\n",
      "   3811         raise InvalidIndexError(key)\n",
      "-> 3812     raise KeyError(key) from err\n",
      "        key = '\"Employee PTO Request for Paid Time Off\"'\n",
      "   3813 except TypeError:\n",
      "   3814     # If we have a listlike key, _check_indexing_error will raise\n",
      "   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
      "   3816     #  the TypeError.\n",
      "   3817     self._check_indexing_error(key)\n",
      "\n",
      "KeyError: '\"Employee PTO Request for Paid Time Off\"'\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(\n",
      "    self=Index(['AB', 'AP', 'AS',\n",
      "       'AZ_x0005_AZ_x00...,\n",
      "      dtype='object', name='State', length=172),\n",
      "    key='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "   3804 try:\n",
      "-> 3805     return self._engine.get_loc(casted_key)\n",
      "        casted_key = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        self = Index(['AB', 'AP', 'AS',\n",
      "       'AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ',\n",
      "       'Abu Dhabi, UAE', 'Al', 'Alabama', 'Alaska',\n",
      "       'Andaman and Nicobar Islands', 'Antigua & Barbuda',\n",
      "       ...\n",
      "       'W Yorkshire', 'WY', 'Washington', 'Washington D.C.',\n",
      "       'West Coast Demerara', 'West Virginia', 'Wexford', 'Wisconsin', 'Worcs',\n",
      "       '北京市'],\n",
      "      dtype='object', name='State', length=172)\n",
      "   3806 except KeyError as err:\n",
      "\n",
      "File index.pyx:167, in pandas._libs.index.IndexEngine.get_loc()\n",
      "\n",
      "File index.pyx:196, in pandas._libs.index.IndexEngine.get_loc()\n",
      "\n",
      "File pandas/_libs/hashtable_class_helper.pxi:7081, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
      "\n",
      "File pandas/_libs/hashtable_class_helper.pxi:7089, in pandas._libs.hashtable.PyObjectHashTable.get_item()\n",
      "\n",
      "KeyError: '\"Employee PTO Request for Paid Time Off\"'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "KeyError                                  Traceback (most recent call last)\n",
      "Cell In[96], line 48, in update_state_bar_chart(\n",
      "    selected_topic='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "     44 # Create bar chart traces for the selected topic and top states\n",
      "     45 traces = []\n",
      "     46 traces.append(go.Bar(\n",
      "     47     x=[selected_topic],\n",
      "---> 48     y=[topic_data[selected_topic]],\n",
      "        traces = []\n",
      "        go = <module 'plotly.graph_objs' from '/Users/muzammil.mahmud/Desktop/pytorch_m1/lib/python3.9/site-packages/plotly/graph_objs/__init__.py'>\n",
      "        selected_topic = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        [selected_topic] = ['\"Employee PTO Request for Paid Time Off\"']\n",
      "        topic_data = State\n",
      "AB                                                          0\n",
      "AP                                                          0\n",
      "AS                                                          0\n",
      "AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ    0\n",
      "Abu Dhabi, UAE                                              0\n",
      "                                                           ..\n",
      "West Virginia                                               0\n",
      "Wexford                                                     0\n",
      "Wisconsin                                                   8\n",
      "Worcs                                                       0\n",
      "北京市                                                         0\n",
      "Name: \"Employee PTO Request for Paid Time Off\", Length: 172, dtype: int64\n",
      "     49     name=selected_topic\n",
      "     50 ))\n",
      "     52 for state in top_states.index:\n",
      "     53     traces.append(go.Bar(\n",
      "     54         x=[selected_topic],\n",
      "     55         y=[topic_data[state]],\n",
      "     56         name=state,\n",
      "     57         showlegend=False\n",
      "     58     ))\n",
      "\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/series.py:1112, in Series.__getitem__(\n",
      "    self=State\n",
      "AB                                        ...est for Paid Time Off\", Length: 172, dtype: int64,\n",
      "    key='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "   1109     return self._values[key]\n",
      "   1111 elif key_is_scalar:\n",
      "-> 1112     return self._get_value(key)\n",
      "        key = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        self = State\n",
      "AB                                                          0\n",
      "AP                                                          0\n",
      "AS                                                          0\n",
      "AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ    0\n",
      "Abu Dhabi, UAE                                              0\n",
      "                                                           ..\n",
      "West Virginia                                               0\n",
      "Wexford                                                     0\n",
      "Wisconsin                                                   8\n",
      "Worcs                                                       0\n",
      "北京市                                                         0\n",
      "Name: \"Employee PTO Request for Paid Time Off\", Length: 172, dtype: int64\n",
      "   1114 # Convert generator to list before going through hashable part\n",
      "   1115 # (We will iterate through the generator there to check for slices)\n",
      "   1116 if is_iterator(key):\n",
      "\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/series.py:1228, in Series._get_value(\n",
      "    self=State\n",
      "AB                                        ...est for Paid Time Off\", Length: 172, dtype: int64,\n",
      "    label='\"Employee PTO Request for Paid Time Off\"',\n",
      "    takeable=False\n",
      ")\n",
      "   1225     return self._values[label]\n",
      "   1227 # Similar to Index.get_value, but we do not fall back to positional\n",
      "-> 1228 loc = self.index.get_loc(label)\n",
      "        label = '\"Employee PTO Request for Paid Time Off\"'\n",
      "        self = State\n",
      "AB                                                          0\n",
      "AP                                                          0\n",
      "AS                                                          0\n",
      "AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ_x0005_AZ    0\n",
      "Abu Dhabi, UAE                                              0\n",
      "                                                           ..\n",
      "West Virginia                                               0\n",
      "Wexford                                                     0\n",
      "Wisconsin                                                   8\n",
      "Worcs                                                       0\n",
      "北京市                                                         0\n",
      "Name: \"Employee PTO Request for Paid Time Off\", Length: 172, dtype: int64\n",
      "   1230 if is_integer(loc):\n",
      "   1231     return self._values[loc]\n",
      "\n",
      "File ~/Desktop/pytorch_m1/lib/python3.9/site-packages/pandas/core/indexes/base.py:3812, in Index.get_loc(\n",
      "    self=Index(['AB', 'AP', 'AS',\n",
      "       'AZ_x0005_AZ_x00...,\n",
      "      dtype='object', name='State', length=172),\n",
      "    key='\"Employee PTO Request for Paid Time Off\"'\n",
      ")\n",
      "   3807     if isinstance(casted_key, slice) or (\n",
      "   3808         isinstance(casted_key, abc.Iterable)\n",
      "   3809         and any(isinstance(x, slice) for x in casted_key)\n",
      "   3810     ):\n",
      "   3811         raise InvalidIndexError(key)\n",
      "-> 3812     raise KeyError(key) from err\n",
      "        key = '\"Employee PTO Request for Paid Time Off\"'\n",
      "   3813 except TypeError:\n",
      "   3814     # If we have a listlike key, _check_indexing_error will raise\n",
      "   3815     #  InvalidIndexError. Otherwise we fall through and re-raise\n",
      "   3816     #  the TypeError.\n",
      "   3817     self._check_indexing_error(key)\n",
      "\n",
      "KeyError: '\"Employee PTO Request for Paid Time Off\"'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "# Preprocess data\n",
    "df_final.dropna(subset=['Email'], inplace=True)\n",
    "df_final.dropna(subset=['Industry Category'], inplace=True)\n",
    "\n",
    "# Define Dash app\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define topics and states\n",
    "topics = df_final['GPT Improved Labels'].unique()\n",
    "states = df_final['State'].unique()\n",
    "\n",
    "# Create an empty DataFrame to hold the frequency counts\n",
    "topic_counts = df_final.groupby(['GPT Improved Labels', 'State']).size().unstack(fill_value=0)\n",
    "\n",
    "# Define layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Interactive Dashboard\"),\n",
    "    dcc.Dropdown(\n",
    "        id='topic-dropdown',\n",
    "        options=[{'label': topic, 'value': topic} for topic in topics],\n",
    "        value=topics[0]  # default value\n",
    "    ),\n",
    "    dcc.Graph(id='topic-state-bar-chart', style={'width': '95%', 'height': '80vh'})\n",
    "])\n",
    "\n",
    "# Define callback to update bar chart based on dropdown selection\n",
    "@app.callback(\n",
    "    Output('topic-state-bar-chart', 'figure'),\n",
    "    [Input('topic-dropdown', 'value')]\n",
    ")\n",
    "def update_state_bar_chart(selected_topic):\n",
    "    # Get frequency counts for the selected topic\n",
    "    topic_data = topic_counts.loc[selected_topic]\n",
    "\n",
    "    # Sort the states based on frequency count\n",
    "    top_states = topic_data.sort_values(ascending=False).head(5)\n",
    "\n",
    "    # Create bar chart traces for the selected topic and top states\n",
    "    traces = []\n",
    "    traces.append(go.Bar(\n",
    "        x=[selected_topic],\n",
    "        y=[topic_data[selected_topic]],\n",
    "        name=selected_topic\n",
    "    ))\n",
    "\n",
    "    for state in top_states.index:\n",
    "        traces.append(go.Bar(\n",
    "            x=[selected_topic],\n",
    "            y=[topic_data[state]],\n",
    "            name=state,\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "    return {\n",
    "        'data': traces,\n",
    "        'layout': go.Layout(\n",
    "            title=f'Distribution of States for Topic: {selected_topic}',\n",
    "            xaxis=dict(title='Topic'),\n",
    "            yaxis=dict(title='Frequency'),\n",
    "            barmode='stack',\n",
    "            height=800\n",
    "        )\n",
    "    }\n",
    "\n",
    "# Run app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
